{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "zKGVfdFwKhuo"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import Dataset\n",
    "import glob\n",
    "import os\n",
    "import zarr\n",
    "import random\n",
    "import datetime\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "from imgaug import augmenters as iaa\n",
    "from imgaug.augmentables.segmaps import SegmentationMapsOnImage\n",
    "from imgaug.augmentables.heatmaps import HeatmapsOnImage\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchsummary import summary\n",
    "from utils.colormap import *\n",
    "from unet_fov import *\n",
    "from utils.mean_shift import MeanShift\n",
    "from skimage import measure\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mPjWemFDKhus"
   },
   "source": [
    "Tile and Stitch\n",
    "==================================\n",
    "\n",
    "When applying one of our deep learning models on microscopy data, it might be that the images are too big to be stored on the GPU and predicted at once. In such situations, the image is subdivided into **tiles**, the **tiles** are fed into the DL model and the respective predictions are stitched together to form an image of the same resolution as the original microscopy image that was subject to inference. \n",
    "\n",
    "For a more detailed reference, please see https://arxiv.org/pdf/2101.05846.pdf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "coGjBzUBKhuu"
   },
   "source": [
    "Data\n",
    "-------\n",
    "For this task we use again a subset of the data used in the kaggle data science bowl 2018 challenge\n",
    "(https://www.kaggle.com/c/data-science-bowl-2018/)\n",
    "\n",
    "![image.png](utils/attachment/image.png)\n",
    "All images show nuclei recorded using different microscopes and lighting conditions.\n",
    "There are 30 images in the training set, 8 in the validation set and 16 in the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#TODO: Create the KaggleDSB_dataset\n",
    "-------\n",
    "We will create the KaggleDSB_dataset, a subclass which inherits from torch.utils.data.Dataset.\n",
    "\n",
    "When you just have limited number of data for training, data augmentation is essential to get good results.\n",
    "\n",
    "TODO: Implement the part of **define_augmentation** for training data during training on the fly.Think about what kind of augmentation to use (e.g. flips, rotation, elastic).Use the imgaug library (https://imgaug.readthedocs.io/en/latest/), it provides a very extensive list of available augmentations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decompress data\n",
    "from shutil import unpack_archive\n",
    "unpack_archive(os.path.join('datasets','data_kaggle.tar.gz'), './')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KaggleDSB_dataset(Dataset):\n",
    "    \"\"\"(subset of the) kaggle data science bowl 2018 dataset.\n",
    "    The data is loaded from disk on the fly and in parallel using the torch dataset class.\n",
    "    This enables the use of datasets that would not fit into main memory and dynamic augmentation.\n",
    "    Args:\n",
    "        root_dir (string): Directory with all the images.\n",
    "        data_type (string): train/val/test, select subset of images\n",
    "        prediction_type (string): default to be \"metric_learning\" for this notebook\n",
    "        net_input_size (list): the input title size of you UNet\n",
    "        padding_size (int): the number of pixels to pad on each side of the image before augmentation and cropping\n",
    "        cache: if cache the data, default: False\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 root_dir,\n",
    "                 data_type,\n",
    "                 prediction_type=\"two_class\",\n",
    "                 net_input_size=None,\n",
    "                 padding_size=None,\n",
    "                 cache = False\n",
    "                ):\n",
    "        self.data_type = data_type\n",
    "        self.files = glob.glob(os.path.join(root_dir, data_type, \"*.zarr\"))\n",
    "        self.prediction_type = prediction_type\n",
    "        self.net_input_size = net_input_size\n",
    "        self.padding_size = padding_size\n",
    "        self.define_augmentation()\n",
    "        self.cache = cache\n",
    "        if cache:\n",
    "            self.cached_data = [self.load_sample(filename) for filename in self.files]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "    \n",
    "    def define_augmentation(self):\n",
    "        \n",
    "        self.transform = iaa.Identity\n",
    "        self.crop = None\n",
    "        self.pad = None\n",
    "\n",
    "        ###########################################################################\n",
    "        # TODO (optional): Define your augmentation pipeline and uncomment the    #\n",
    "        # following code                                                          #\n",
    "        ###########################################################################\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        \n",
    "        # define self.transfrom by looking into the imgaug package reference\n",
    "        \n",
    "        # self.transform = iaa.Sequential([\n",
    "        #     ...,\n",
    "        #     ...,\n",
    "        #    ...\n",
    "        # ], random_order=True)\n",
    "        \n",
    "        \n",
    "        # if self.net_input_size is not None:\n",
    "        #     self.crop = ... which augmentation?\n",
    "\n",
    "        # if self.padding_size is not None:\n",
    "        #     self.pad =  ... which augmentation\n",
    "                    \n",
    "            \n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        ###########################################################################\n",
    "        #                             END OF YOUR CODE                            #\n",
    "        ###########################################################################\n",
    "        \n",
    "    def get_filename(self, idx):\n",
    "        return self.files[idx]\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        if self.cache:\n",
    "            raw, label = self.cached_data[idx]\n",
    "        else:\n",
    "            fn = self.get_filename(idx)\n",
    "            raw, label = self.load_sample(fn)\n",
    "        raw = self.normalize(raw)\n",
    "        # augment for training\n",
    "        if self.padding_size is not None:\n",
    "            raw = self.pad(images = raw) # CHW -> CHW\n",
    "            label = self.pad(images = label) # CHW -> CHW\n",
    "        if self.data_type == \"train\":\n",
    "            raw = np.transpose(raw, [1,2,0]) # CHW -> HWC\n",
    "            label = np.transpose(label, [1,2,0]) # CHW -> HWC            \n",
    "            raw, label = self.augment_sample(raw, label) # HWC -> HWC\n",
    "            raw = np.transpose(raw, [2,0,1]) # HWC -> CHW\n",
    "            label = np.transpose(label, [2,0,1]) # HWC -> CHW\n",
    "        if self.net_input_size is not None:\n",
    "            tmp = np.concatenate([raw, label], axis = 0).copy() # C1+C2 HW\n",
    "            tmp = np.transpose(tmp, [1,2,0]) # CHW -> HWC \n",
    "            tmp = self.crop.augment_image(tmp) # HWC -> HWC\n",
    "            tmp = np.transpose(tmp, [2,0,1])\n",
    "            raw, label = np.expand_dims(tmp[0], axis=0), np.stack(tmp[1:],axis=0) # split\n",
    "        raw, label = torch.Tensor(raw), torch.Tensor(label)\n",
    "        return raw, label\n",
    "    \n",
    "    def augment_sample(self, raw, label):\n",
    "        # stores float label (sdt) differently than integer label (rest)\n",
    "        if self.prediction_type in [\"sdt\"]:\n",
    "            label = HeatmapsOnImage(label, shape=raw.shape, min_value=-1.0, max_value=1.0)\n",
    "            raw, label = self.transform(image=raw, heatmaps=label)\n",
    "        else:\n",
    "            label = label.astype(np.int32)\n",
    "            label = SegmentationMapsOnImage(label, shape=raw.shape)\n",
    "            raw, label = self.transform(image=raw, segmentation_maps=label)\n",
    "            \n",
    "        label = label.get_arr() \n",
    "        # some pytorch version have problems with negative indices introduced by e.g. flips\n",
    "        # just copying fixes this\n",
    "        label = label.copy()\n",
    "        raw = raw.copy()\n",
    "        return raw, label\n",
    "    \n",
    "    def normalize(self, raw):\n",
    "        # z-normalization\n",
    "        raw -= np.mean(raw)\n",
    "        raw /= np.std(raw)\n",
    "        return raw\n",
    "    \n",
    "    def load_sample(self, filename):\n",
    "        data = zarr.open(filename)\n",
    "        raw = np.array(data['volumes/raw'])\n",
    "        if self.prediction_type == \"two_class\":\n",
    "            label = np.array(data['volumes/gt_fgbg'])\n",
    "        elif self.prediction_type == \"affinities\":\n",
    "            label = np.array(data['volumes/gt_affs'])\n",
    "        elif self.prediction_type == \"sdt\":\n",
    "            label = np.array(data['volumes/gt_tanh'])\n",
    "        elif self.prediction_type == \"three_class\":\n",
    "            label = np.array(data['volumes/gt_threeclass'])\n",
    "        elif self.prediction_type == \"metric_learning\":\n",
    "            label = np.array(data['volumes/gt_labels'])\n",
    "        label = label.astype(np.float32)\n",
    "        return raw, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x_Z4xiRUKhuy"
   },
   "source": [
    "Loss\n",
    "-------\n",
    "Here we will just use the metric learning approach with the well-known discriminative loss that you are already familiarized with in the instance segmentation part of the exercises.\n",
    "\n",
    "### #TODO: Metric Learning ###\n",
    "In metric learning your model learns to predict an embedding vector for each pixel. These embedding vectors are learned such that vectors from pixels belonging to the same instance are similar to each other and dissimilar to the embedding vectors of other instances and the background. It can also be thought of as learning a false coloring where each instance is colored with a unique but arbitrary color.  \n",
    "![metric_learning.png](utils/attachment/metric_learning.png)\n",
    "\n",
    "TODO: Please fill in the missing code according to the knowledge you learnt from the instance_segmentation.ipynb exercise.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hHMR6rMXKhu0"
   },
   "outputs": [],
   "source": [
    "from utils.disc_loss import DiscriminativeLoss\n",
    "\n",
    "#hint: see here for torch tensor types: https://pytorch.org/docs/stable/tensors.html\n",
    "\n",
    "prediction_type = \"metric_learning\"\n",
    "###########################################################################\n",
    "# TODO                                                                    #\n",
    "###########################################################################\n",
    "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "out_channels = \n",
    "activation = \n",
    "loss_fn = \n",
    "dtype = \n",
    "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "###########################################################################\n",
    "#                             END OF YOUR CODE                            #\n",
    "###########################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JixvL0eNKhu1"
   },
   "source": [
    "Create our input datasets, ground truth labels are chosen depending on the type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TnnMZY29Khu2"
   },
   "outputs": [],
   "source": [
    "# make datasets\n",
    "root = 'data_kaggle_test'\n",
    "padding_size = 46\n",
    "batch_size = 1\n",
    "net_input_size = [332,332]\n",
    "\n",
    "data_train = KaggleDSB_dataset(root, \"train\", prediction_type=prediction_type, padding_size=padding_size, net_input_size=net_input_size, cache=False)\n",
    "data_val = KaggleDSB_dataset(root, \"val\", prediction_type=prediction_type, padding_size=padding_size,net_input_size=net_input_size, cache=False)\n",
    "data_test = KaggleDSB_dataset(root, \"test\", prediction_type=prediction_type, padding_size=padding_size)\n",
    "# make dataloaders\n",
    "train_loader = DataLoader(data_train, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "val_loader = DataLoader(data_val, batch_size=1, pin_memory=True)\n",
    "test_loader = DataLoader(data_test, batch_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9ekmrUSuKhu3"
   },
   "source": [
    "Let's have a look at some of the raw data and labels:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TlzNfKL-Khu4",
    "outputId": "0a495f5f-5c36-4f7b-dcb8-b924209c8513"
   },
   "outputs": [],
   "source": [
    "# repeatedly execute this cell to get different images\n",
    "for image, label in data_train:\n",
    "    break\n",
    "\n",
    "label = np.squeeze(label, 0)\n",
    "if prediction_type == \"affinities\":\n",
    "    label = label[0] + label[1]\n",
    "\n",
    "fig=plt.figure(figsize=(12, 8))\n",
    "fig.add_subplot(1, 2, 1)\n",
    "plt.imshow(np.squeeze(image), cmap='gray')\n",
    "fig.add_subplot(1, 2, 2)\n",
    "plt.imshow(np.squeeze(label), cmap='gist_earth')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S6XJq6waKhu5"
   },
   "source": [
    "#TODO: Define our U-Net\n",
    "==============\n",
    "As before, we define our neural network architecture and can choose the depth and number of feature maps at the first convolution. Our CNN used 'same' padding before, which assures that spatial dimensions are not reduced by the convolution operation. Unfortunately, this padding scheme renders the network location-aware, since it can learn to calculate the distance of most pixels in the image to the padded zeros at the image boundaries. This location awareness then leads to discontinuities at the stitching boundaries. Therefore we'll use 'valid' padding instead of 'same' padding.\n",
    "\n",
    "Valid padding actually means that input filter maps are not padded at all and therefore the spatial dimensions of the filter maps reduce after every convolution. This leads to the network predicting smaller tiles than the ones that got fed into the network as you can see in the network summary below, e.g choosing the net_input_size = $[332,332]$, our output tile size will be $[148,148]$ with our defined UNet structure. Therefore the input tiles need to overlap such that the output tiles align.\n",
    "\n",
    "TODO:You need to build a Unet with the following conditions, otherwise you will run into errors when running the next cell.\n",
    "- 1. Use valid padding.\n",
    "- 2. The number of feature maps in the first layer should be 32 as well as the number of out feature maps.\n",
    "- 3. The feature maps should go through 4 times downsampling and 4 times upsampling with the scale factor=2.\n",
    "- 4. fmap_inc_factor which refers to the number of feature maps between layers should be set to 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unet_fov import UNet\n",
    "\n",
    "torch.manual_seed(42)\n",
    "net_input_size = [332,332]\n",
    "\n",
    "# hint: see network requirements above and torch.nn.Sequential for building network\n",
    "# hint: we defined out channels in a previous cell. how many output channels do we get from the Network?\n",
    "# hint: how can we get to the right number of output channels? \n",
    "\n",
    "###########################################################################\n",
    "# TODO: Define the net and uncomment the following code                   #\n",
    "# Please define a UNet which use valid padding                             #\n",
    "###########################################################################\n",
    "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "# d_factors = \n",
    "# net = torch.nn.Sequential(\n",
    "#     UNet(in_channels=,\n",
    "#     num_fmaps=,\n",
    "#     fmap_inc_factors=,\n",
    "#     downsample_factors=d_factors,\n",
    "#     activation='ReLU',\n",
    "#     padding=,\n",
    "#     num_fmaps_out=,\n",
    "#     constant_upsample=False\n",
    "#     ),\n",
    "#     torch.nn.Conv2d(in_channels=, out_channels=out_channels, kernel_size=1, padding=0, bias=True))\n",
    "\n",
    "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "###########################################################################\n",
    "#                             END OF YOUR CODE                            #\n",
    "###########################################################################\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "net = net.to(device)\n",
    "summary(net, (1, net_input_size[0], net_input_size[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-k34Q3KEKhu6"
   },
   "source": [
    "Training\n",
    "=======\n",
    "\n",
    "You already know the training loop from the other notebooks. For this execise we already trained a network like the one that you just defined for 60,000 training steps. You can load this CNN by executing the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7k0lYFXpKhu7"
   },
   "outputs": [],
   "source": [
    "#load pretrained network (checkpoint unet_60000)\n",
    "\n",
    "net=torch.load(\"utils/unet_60000\",map_location=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wM_kcITMKhu7"
   },
   "source": [
    "Postprocessing\n",
    "=============\n",
    "\n",
    "Here we first tile the input image and feed the tiles to the network to generate our predictions. Then the tiles are cropped and stitched to form the final embeddings. In the lecture you learned that U-Nets are shift equivariant only for shifts that are multiples of $f^l$, since our network does pooling with window size and stride $2$ and has $4$ pooling layers $f^l=2^4=16$. When stitching output crops of this U-Net whose spatial dimensions are not multiples of $16$ you might observe discontinuities at the stitching boundaries that lead to false splits. \n",
    "\n",
    "Try running the below cell and then try the questions in the following cell!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_73WPKbKKhu7",
    "outputId": "c391b213-fe62-43df-ba1b-0b907f329382",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "from utils.label import *\n",
    "from utils.evaluate import *\n",
    "\n",
    "crop_size =  [50, 50]\n",
    "output_size = [148,148]\n",
    "padding_size = 0\n",
    "data_test = KaggleDSB_dataset(root, \"test\", prediction_type=prediction_type)\n",
    "test_loader = DataLoader(data_test, batch_size=1)\n",
    "\n",
    "# set flag\n",
    "net.eval()\n",
    "# set hyperparameters\n",
    "prediction_type == \"metric_learning\"\n",
    "fg_thresh = 0.7\n",
    "seed_thresh = None\n",
    "    \n",
    "def unpad(pred, padding_size):\n",
    "    return pred[padding_size:-padding_size,padding_size:-padding_size]\n",
    "\n",
    "avg = 0.0\n",
    "pad_top_left = (net_input_size[0]-output_size[0])//2\n",
    "for idx, (image, gt_labels) in enumerate(test_loader):\n",
    "    image = image.to(device)\n",
    "    if crop_size:\n",
    "        output_size = crop_size\n",
    "    image_padded = F.pad(image, (pad_top_left,300,pad_top_left,300))\n",
    "    H,W = image.shape[-2:]\n",
    "    patched_pred = torch.zeros(1,4,image.shape[2]+200,image.shape[3]+200)\n",
    "    for h in range(0, H, output_size[0]):\n",
    "        for w in range(0, W, output_size[1]):\n",
    "            image_tmp = image_padded[:,:,h:h+net_input_size[0],w:w+net_input_size[1]]\n",
    "            pred = net(image_tmp)\n",
    "            if crop_size:\n",
    "                pred = pred[:,:,:crop_size[0],:crop_size[1]]\n",
    "            patched_pred[:,:,h:h+pred.shape[2],w:w+pred.shape[3]] = pred\n",
    "    pred = patched_pred[:,:,0:360,0:360]\n",
    "    image = np.squeeze(image.cpu())\n",
    "    gt_labels = np.squeeze(gt_labels)\n",
    "    pred = np.squeeze(pred.cpu().detach().numpy(),0)\n",
    "    #if prediction_type in [\"three_class\", \"affinities\",\"two_class\",\"sdt\"]:\n",
    "    if padding_size and padding_size>0:\n",
    "        pred = unpad(np.transpose(pred,(1,2,0)), padding_size)\n",
    "        pred = np.transpose(pred,(2,0,1))\n",
    "    labelling, surface = label(pred, prediction_type, fg_thresh=fg_thresh, seed_thresh=seed_thresh)\n",
    "    ap, precision, recall, tp, fp, fn = evaluate(labelling, data_test.get_filename(idx))\n",
    "    avg += ap\n",
    "    print(np.min(surface), np.max(surface))\n",
    "    labelling = labelling.astype(np.uint8)\n",
    "    print(\"average precision: {}, precision: {}, recall: {}\".format(ap, precision, recall))\n",
    "    print(\"true positives: {}, false positives: {}, false negatives: {}\".format(tp, fp, fn))\n",
    "    if prediction_type == \"metric_learning\":\n",
    "        surface = surface+np.abs(np.min(surface, axis=(1,2)))[:,np.newaxis,np.newaxis]\n",
    "        surface /= np.max(surface, axis=(1,2))[:,np.newaxis,np.newaxis]\n",
    "        surface = np.transpose(surface, (1,2,0))\n",
    "    \n",
    "    fig=plt.figure(figsize=(16, 8))\n",
    "    ax = fig.add_subplot(1, 4, 1)\n",
    "    ax.set_title(\"raw\")\n",
    "    plt.imshow(np.squeeze(image))\n",
    "    ax = fig.add_subplot(1, 4, 2)\n",
    "    ax.set_title(\"gt labels\")\n",
    "    plt.imshow(np.squeeze(1.0-gt_labels))\n",
    "    \n",
    "    ax = fig.add_subplot(1, 4, 3)\n",
    "    ax.set_title(\"prediction\")\n",
    "    plt.imshow(np.squeeze(1.0-surface))\n",
    "    ax = fig.add_subplot(1, 4, 4)\n",
    "    ax.set_title(\"pred segmentation\")\n",
    "    plt.imshow(np.squeeze(labelling), cmap=rand_cmap, interpolation=\"none\")\n",
    "    bs = output_size[0]\n",
    "    lw = 1\n",
    "    for k in range(bs, labelling.shape[-1], bs):\n",
    "        of = -0.5\n",
    "        ax.plot([k+of, k+of], [0, labelling.shape[-2]-1], color='red', linestyle='--', dashes=(5, 10), linewidth=lw)\n",
    "    for l in range(bs, labelling.shape[-2], bs):\n",
    "        of = -0.5\n",
    "        ax.plot([0, labelling.shape[-1]-1], [l+of, l+of], color='red', linestyle='--', dashes=(5, 10),linewidth=lw)\n",
    "\n",
    "    break\n",
    "    plt.show()\n",
    "avg /= (idx+1)\n",
    "print(\"average precision on test set: {}\".format(avg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try changing the crop_size above and compare the different results.\n",
    "# What happens when we use a crop size that is a multiple of 16?\n",
    "# What happens when we use a low crop size (e.g [20,20])? Why?\n",
    "# Try iterating over full data loader and see different results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# solutions\n",
    "\n",
    "# we see discontinuties at the stitching boundaries if the crop size is a multiple of 16\n",
    "# we get out of memory errors on the gpu with a low crop size - iterating over more patches\n",
    "# remove break statement"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "tile_and_stitch.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "cff0e7dafc57c8272ee1343dbc442356272010b158bded75ebcd02aba72f7245"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
